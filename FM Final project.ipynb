{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fpmul\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:37: UserWarning: Unsupported `ReduceOp` for distributed computing.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1234"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Test PyTorch installation\n",
    "import torch \n",
    "import pytorch_lightning as pl\n",
    "\n",
    "pl.seed_everything(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "class MagicCards(pl.LightningDataModule):\n",
    "    \"\"\" A datamodule for the RNN. \"\"\"\n",
    "    def __init__(self, dict_size=77, batch_size=128, loc=\"./data/minitrain.txt\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dict_size = dict_size\n",
    "        self.batch_size = batch_size\n",
    "        self.example_length = None\n",
    "        self.loc = loc\n",
    "\n",
    "        # preprocess training data\n",
    "        self.tok = Tokenizer(num_words=dict_size, char_level=True, filters=\"\", lower=False,)\n",
    "\n",
    "    def load_data(self, location):\n",
    "        with open(location, 'rt') as f:\n",
    "            return f.read()\n",
    "    \n",
    "    def make_dataset(self, documents, tok=None):\n",
    "        # tokenize\n",
    "        sequences = self.tok.texts_to_sequences(documents)\n",
    "        \n",
    "        x = sequences\n",
    "        y = [sequence[1:] for sequence in sequences]\n",
    "        #Now we pad. We post pad here because we will be generating sequentially\n",
    "        #and do not want to get squeezed\n",
    "        x = sequence.pad_sequences(x, padding=\"post\")\n",
    "        self.example_length = len(x[0])\n",
    "        y = sequence.pad_sequences(y, padding=\"post\", maxlen=self.example_length)\n",
    "        \n",
    "        \n",
    "        # make torch arrays.\n",
    "        x = torch.from_numpy(x).to(torch.int64)\n",
    "        y = torch.from_numpy(y).to(torch.int64)\n",
    "        \n",
    "        return TensorDataset(x, y)\n",
    "\n",
    "    def setup(self, stage):\n",
    "        # load data\n",
    "        train_seqs = self.load_data(self.loc).split(\"\\n\\n\")\n",
    "        \n",
    "        # fit tokenizer\n",
    "        self.tok.fit_on_texts(train_seqs)\n",
    "        \n",
    "        # make datasets\n",
    "        self.train = self.make_dataset(train_seqs)\n",
    "        self.test = self.train\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=self.batch_size, shuffle=True,\n",
    "                          num_workers=mp.cpu_count() // 4)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test, batch_size=self.batch_size, shuffle=False,\n",
    "                          num_workers=mp.cpu_count() // 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_onehot(inp, dict_length=None):\n",
    "    inp = np.array(inp)\n",
    "    if dict_length == None:\n",
    "        dict_length = inp.max+1\n",
    "    \n",
    "    one_hot = np.zeros((inp.size, dict_length))\n",
    "    one_hot[np.arange(inp.size), inp] = 1\n",
    "    \n",
    "    return one_hot\n",
    "\n",
    "class RNN(pl.LightningModule):\n",
    "    \"\"\" Baseline RNN classifier \"\"\"\n",
    "\n",
    "    def __init__(self, dict_size=77, example_length=660, lstm_layers=1, dropout=0.5, rnn_width=256):\n",
    "        \"\"\"\n",
    "        initialize RNN model\n",
    "        :param embedding_length: size of word embedding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # To complete this function, you will need PyTorch's Embedding, LSTM and linear layers.\n",
    "        \n",
    "        # The embedding layer simply creates a dictionary between words in your vocabulary and their vector \n",
    "        # representations. Therefore, each word has a unique representation.\n",
    "        # For instance, say your input x is encoded as [1, 5, 9] and embedding_dim = 32 (see documentation for \n",
    "        # arguments to this layer), then after passing through the embedding layer the output will be \n",
    "        # of shape 3x32\n",
    "        \n",
    "        # Documentation for LSTM layer in :\n",
    "        #     https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM\n",
    "        \n",
    "        # Note there are multiple ways to define your model, \n",
    "        # we suggest adding individual layers here, but any method is fine.\n",
    "        \n",
    "        # As an example, you could define a linear layer with n inputs and m outputs like so:\n",
    "        # self.linear = nn.Linear(n, m)\n",
    "        \n",
    "        # Similarly, define the three layers needed for your model\n",
    "        \n",
    "        # TODO: build model by defining individual layers in the network\n",
    "            \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        self.dict_size = dict_size\n",
    "        self.num_layers = lstm_layers\n",
    "        self.lstm_size = rnn_width\n",
    "        self.emb_layer = nn.Embedding(dict_size, rnn_width)\n",
    "        self.lstm_layer = nn.LSTM(rnn_width, rnn_width,\n",
    "                                  batch_first=True,\n",
    "                                  num_layers=lstm_layers,\n",
    "                                  dropout=dropout)\n",
    "        self.lin_layer = nn.Linear(rnn_width, dict_size)\n",
    "        \n",
    "        self.save_hyperparameters('dict_size', 'rnn_width')\n",
    "\n",
    "        self.example_input_array = torch.zeros([1, example_length], dtype=torch.int64)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        \n",
    "        Pytorch allows you to stack layers on top of each other very easily.\n",
    "        \n",
    "        For example, if we have layers self.layer1 and self.layer2 acting on input x, we can do\n",
    "        out_1 = self.layer1(x)\n",
    "        out_2 = self.layer2(out_1)\n",
    "        This would constitute a forward pass for the above hypothetical network.\n",
    "        \n",
    "        Your job in this function is to propagate the input x through the network you defined in __init__()\n",
    "        \n",
    "        TODO:\n",
    "        1. Pass input though embedding layer\n",
    "        2. Propagate output of previous step through LSTM\n",
    "        3. Pass final output of LSTM through linear layer\n",
    "        4. Apply Sigmoid activation (torch.sigmoid()) to output of step 3 to obtain probabilities\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        embed = self.emb_layer(x)\n",
    "        lstm_out, _ = self.lstm_layer(embed)\n",
    "        lin_out = self.lin_layer(lstm_out)\n",
    "        return lin_out\n",
    "\n",
    "    def accuracy(self, y_hat, y):\n",
    "        return (y == y_hat).to(torch.float32).mean()\n",
    "    \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\" Perfom a training step. \n",
    "        \n",
    "        This is just one step on one batch during training (no looping required) \n",
    "        \n",
    "        TODO:\n",
    "            - forward pass on data in batch\n",
    "            - compute training loss (use PyTorch's F.binary_cross_entropy since this is binary classification)\n",
    "            - Compute training accuracy (using the self.accuracy function)\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        x,y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        m = nn.CrossEntropyLoss()\n",
    "        predicts = torch.argmax(F.softmax(y_hat), dim=2)\n",
    "        loss = m(y_hat.view(-1,self.dict_size), y.view(-1))\n",
    "        # We implemented logging for you. \n",
    "        result = pl.TrainResult(loss)\n",
    "        \n",
    "        acc = self.accuracy(predicts.view(-1), y.view(-1))\n",
    "        result.log('train_loss', loss, prog_bar=True)\n",
    "        result.log('train_accuracy', acc, prog_bar=True)\n",
    "        #print(result)\n",
    "        return result\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\" Perfom a test step \n",
    "            hint: your code should be the same as your train step\n",
    "        \n",
    "        TODO:\n",
    "            - forward pass on data in batch\n",
    "            - compute test loss \n",
    "            - Compute test accuracy  \n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        x,y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        m = nn.CrossEntropyLoss()\n",
    "        predicts = torch.argmax(y_hat, dim=2)\n",
    "        \n",
    "        loss = m(y_hat.view(-1,self.dict_size), y.view(-1))\n",
    "        # We implemented logging for you. \n",
    "        result = pl.EvalResult(loss)\n",
    "        acc = self.accuracy(predicts.view(-1), y.view(-1))\n",
    "        result.log('test_loss', loss, prog_bar=True)\n",
    "        result.log('test_accuracy', acc, prog_bar=True)\n",
    "        #print(result)\n",
    "        return result\n",
    "    \n",
    "    def init_state(self, sequence_length):\n",
    "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size),\n",
    "                torch.zeros(self.num_layers, sequence_length, self.lstm_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_cards_dm = MagicCards()\n",
    "\n",
    "def run_rnn(**kwargs):\n",
    "    # helper function for running RNN.\n",
    "    logger = CSVLogger(\"logs\", name=\"rnn\")\n",
    "    trainer = pl.Trainer(\n",
    "        gpus=int(torch.cuda.is_available()),\n",
    "        logger=logger,\n",
    "        min_epochs=5,\n",
    "        max_epochs=100,\n",
    "        row_log_interval=1,\n",
    "        log_save_interval=1,\n",
    "        deterministic=True\n",
    "    )\n",
    "    \n",
    "    model = RNN(**kwargs)\n",
    "    \n",
    "    trainer.fit(model, datamodule=magic_cards_dm)\n",
    "    results = trainer.test(verbose=True)\n",
    "    return logger.experiment.metrics_file_path\n",
    "\n",
    "metrics = run_rnn()\n",
    "#print(results)\n",
    "#print('Accuracy for LSTM: ', results['test_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dm, model, seed_text=None, max_length=None):\n",
    "    model.eval()\n",
    "    \n",
    "    if seed_text:\n",
    "        sequence = np.array(dm.tok.texts_to_sequences(seed_text))\n",
    "    else:\n",
    "        #If we want pure generation, need to just seed the startingin field post\n",
    "        sequence = np.array(dm.tok.texts_to_sequences('|'))\n",
    "\n",
    "    \n",
    "    state_h, _ = model.init_state(len(sequence))\n",
    "    \n",
    "    if max_length:\n",
    "        print(max_length)\n",
    "        for i in range(max_length):\n",
    "            \n",
    "            x = torch.from_numpy(sequence).to(torch.int64)\n",
    "            y_pred = model(x.view(1,-1))\n",
    "            p = torch.nn.functional.softmax(y_pred[0][-1], dim=0).detach().numpy()\n",
    "            char_index = np.random.choice(len(y_pred[0][-1]), p=p)\n",
    "            sequence = np.append(sequence, char_index)\n",
    "            \n",
    "    else:\n",
    "        while sequence[-1] != 0:\n",
    "            x = torch.from_numpy(sequence).to(torch.int64)\n",
    "            y_pred = model(x.view(1,-1))\n",
    "\n",
    "            last_word_logits = y_pred[0][-1]\n",
    "            p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().numpy()\n",
    "            char_index = np.random.choice(len(y_pred[0][-1]), p=p)\n",
    "            sequence = np.append(sequence, char_index)\n",
    "\n",
    "    \n",
    "    output = dm.tok.sequences_to_texts(sequence.reshape(1,-1))\n",
    "    \n",
    "    return output\n",
    "\n",
    "sampler = RNN.load_from_checkpoint(\"logs/rnn/version_103/checkpoints/epoch=99.ckpt\",\n",
    "                                   hparams_file=\"logs/rnn/version_103/hparams.yaml\")\n",
    "samplet = \"\"\n",
    "for i in range(10):\n",
    "  print(\"card: \"+str(i))\n",
    "  sample = predict(magic_cards_dm, sampler, seed_text=\"|\")\n",
    "  str1 = ''\n",
    "  for j in range(0,len(sample[0]),2):\n",
    "    str1 += sample[0][j]\n",
    "\n",
    "  samplet += str1 + \"\\n\\n\"\n",
    "\n",
    "file = open(\"output.txt\", \"w\")\n",
    "file.write(samplet)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
